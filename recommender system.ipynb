{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender System\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Recommender system is a system widely used in Amazon, Netflix to predict user rating for a given item. Usually the system involves collaborative filtering, content-based filtering, session-based filtering or even a mixture.\n",
    "\n",
    "* Collaborative filtering seeks connections across different users and items to predict the rating. It is more related to unsupervised learning. Even inside collaborative filtering, there are a few different techniques such as model-based (matrix factorization, latent variables), memory-based (baseline, KNN) and even a hybrid of both. Collaborative filtering will be the main focus of this script. \n",
    "* Content-based filtering collects user and item profile. Based upon the features in the profile, it forecasts the user preference towards different items. It is more related to supervised learning.\n",
    "* Session-based filtering monitors the user interaction within a session to create recommendations. It is quite helpful to navigate through the cold start problem where a new user has not much available information for modelling. It is more related to deep learning, in particular, Recurrent Neural Network.\n",
    "\n",
    "In a recommender system, not every customer rates every item so it effectively forms a partially filled customer vs item matrix. To recommend anything to the existing customer, a fully filled matrix must be inferred from the dataset. This falls into the spectrum of matrix completion. The most popular sub-problem of matrix completion is to find a low rank matrix via convex optimization (Candès and Recht, 2008). The sub-problem assumes there must be latent variables influencing the users and the items in the matrix. Thus, the matrix must be low rank.\n",
    "\n",
    "Reference to image recovery style matrix completion\n",
    "\n",
    "https://github.com/je-suis-tm/machine-learning/blob/master/matrix%20completion.ipynb\n",
    "\n",
    "Reference to surprise library for recommender system (this script draws a lot of inspiration from it)\n",
    "\n",
    "https://surprise.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('K:/ecole/github/televerser/matrix completion')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data comes from the link below\n",
    "# http://files.grouplens.org/datasets/movielens/ml-100k/u.data\n",
    "# https://github.com/je-suis-tm/machine-learning/blob/master/data/movielens.csv\n",
    "df=pd.read_csv('movielens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#shrink data size for better performance\n",
    "df=df[df['user']<=100][df['item']<=500].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert array into matrix form\n",
    "matrix=df.pivot(index='item',columns='user',values='rating')\n",
    "arr=np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filled elements\n",
    "known_values=list(zip(np.where(~np.isnan(arr))[0],\n",
    "        np.where(~np.isnan(arr))[1]))\n",
    "\n",
    "#randomly select 30% as testing set\n",
    "lucky_draw=set(np.random.choice(range(len(known_values)),\n",
    "                 size=int(len(known_values)*0.3),\n",
    "                 p=[1/len(known_values)]*len(known_values)))\n",
    "\n",
    "unlucky_draw=[i for i in range(len(known_values)) if i not in lucky_draw]\n",
    "testing_idx=(np.where(~np.isnan(arr))[0][list(lucky_draw)],\n",
    "np.where(~np.isnan(arr))[1][list(lucky_draw)])\n",
    "training_idx=(np.where(~np.isnan(arr))[0][list(unlucky_draw)],\n",
    "np.where(~np.isnan(arr))[1][list(unlucky_draw)])\n",
    "\n",
    "#train test split\n",
    "mask=np.ones(arr.shape)\n",
    "mask[testing_idx]=np.nan\n",
    "arr_train=np.multiply(arr,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based\n",
    "\n",
    "#### Funk SVD\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Simon Funk developed a SVD-like latent factor model to win 3rd prize in 2006 Netflix problem. Conventionally people call it Funk SVD, although it is merely inspired by Singular Value Decomposition without explicitly using SVD. Funk SVD is really easy to implement and quick to converge with high accuracy. It is a type of collaborative filtering focusing on matrix factorization. The official optimization problem is formulated as below.\n",
    "\n",
    "$$ \\min_{p_*,q_*,b_*}\\,\\sum_{r_{ui}\\,\\in\\,\\mathcal{K}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 + \\lambda( ||p_u||^2 + ||q_i||^2 + b_u^2 + b_i^2 )$$\n",
    "\n",
    "where \n",
    "\n",
    "${r}_{ui}$ denotes the rating of item $i$ by user $u$\n",
    "\n",
    "$\\hat{r}_{ui}$ denotes the estimated rating of item $i$ by user $u$, it can be decomposed into the form of $\\mu + b_u + b_i + p_u^Tq_i$\n",
    "\n",
    "$\\mathcal{K}$ denotes the observed user vs item matrix (partially filled)\n",
    "\n",
    "$\\mu$ denotes overall average rating\n",
    "\n",
    "$b_u$ denotes the deviation from overall average rating caused by user $u$, can be seen as user baseline where some users are more critical so they give lower ratings in general\n",
    "\n",
    "$b_i$ denotes the deviation from overall average rating caused by item $i$, can be seen as item baseline where some items are more appealing so they attract higher ratings in general\n",
    "\n",
    "$p_u$ denotes the latent factors of user $u$, in translation, user preference\n",
    "\n",
    "$q_i$ denotes the latent factors of item $i$, in translation, item attributes\n",
    "\n",
    "$\\lambda$ denotes LaGrange multiplier which is the coefficient of L2 penalty function\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "To explain a bit, Funk SVD assumes that both user and item are affected by $m$ number of latent factors ($p_u \\in \\mathbb{R}^{m}\\,\\&\\,q_i \\in \\mathbb{R}^{m}$). These latent factors could be percentage of action in the movie, number of tier 1 hollywood stars, etc. The rating $r_{ui}$ is merely the linear combination $p_u^Tq_i$ of user preference and item attributes. This decomposition is similar to SVD in the form of $U\\Sigma V^T$ without the eigenvalue diagonal $\\Sigma$ as scaling factors. Hence, the first bit of the objective function is an ordinary least square to minimize the sum of squared error between existing rating and estimated rating. Since the matrix is partially filled, the second bit of the objective function is L2 norm regularization on user preference $p_u$, item attributes $q_i$, user deviation $b_u$ and item deviation $b_i$ to avoid overfit problem.\n",
    "\n",
    "The actual optimization solver is inspired by gradient descent $\\theta:=\\theta-\\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta}$. Applying the same logic to every unknown parameters $p_u$, $q_i$, $b_u$ and $b_i$, we end up with the following updates.\n",
    "\n",
    "$$b_u := b_u + \\alpha (\\epsilon_{ui} - \\lambda b_u)$$\n",
    "$$b_i := b_i + \\alpha (\\epsilon_{ui} - \\lambda b_i)$$\n",
    "$$p_u := p_u + \\alpha (\\epsilon_{ui} \\cdot q_i - \\lambda p_u)$$\n",
    "$$q_i := q_i + \\alpha (\\epsilon_{ui} \\cdot p_u - \\lambda q_i)$$\n",
    "\n",
    "where\n",
    "\n",
    "$\\epsilon_{ui}$ denotes the error between existing rating and estimated rating in the form of $r_{ui} - \\hat{r}_{ui}$\n",
    "\n",
    "$\\alpha$ denotes the learning rate of gradient descent which dictates how soon the solver reaches the local optima\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Reference to Singular Value Decomposition\n",
    "\n",
    "https://github.com/je-suis-tm/machine-learning/blob/master/principal%20component%20analysis.ipynb\n",
    "\n",
    "Reference to Gradient Descent\n",
    "\n",
    "https://github.com/je-suis-tm/machine-learning/blob/master/gradient%20descent.ipynb\n",
    "\n",
    "Reference to Simon Funk's blog\n",
    "\n",
    "https://sifter.org/~simon/journal/20061211.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use numba to dramatically boost the speed of linear algebra\n",
    "#this will be a lot faster than surprise library\n",
    "@numba.njit\n",
    "def funk_svd_epoch(arr,miu,b_u,b_i,p_u,q_i,alpha,lambda_):\n",
    "    \n",
    "    #initialize\n",
    "    error=0\n",
    "    \n",
    "    #only iterate known ratings\n",
    "    for i in range(arr.shape[0]):\n",
    "        for u in range(arr.shape[1]):\n",
    "            r_ui=arr[i,u]\n",
    "            \n",
    "            #another way to identify nan\n",
    "            #r_ui!=r_ui\n",
    "            if np.isnan(r_ui):\n",
    "                continue\n",
    "\n",
    "            #compute error\n",
    "            epsilon_ui=r_ui-miu-b_u[u]-b_i[i]-p_u[u].T@q_i[i]\n",
    "            error+=epsilon_ui\n",
    "\n",
    "            #update\n",
    "            b_u[u]+=alpha*(epsilon_ui-lambda_*b_u[u])\n",
    "            b_i[i]+=alpha*(epsilon_ui-lambda_*b_i[i])\n",
    "            p_u[u]+=alpha*(epsilon_ui*q_i[i]-lambda_*p_u[u])\n",
    "            q_i[i]+=alpha*(epsilon_ui*p_u[u]-lambda_*q_i[i])\n",
    "    \n",
    "    return error,b_u,b_i,p_u,q_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svd inspired latent factor model by simon funk\n",
    "def funk_svd(arr,miu_init=None,b_u_init=[],b_i_init=[],\n",
    "             p_u_init=[],q_i_init=[],num_of_rank=40,\n",
    "             alpha=0.005,lambda_=0.02,tau=0.0001,\n",
    "             max_iter=20,diagnosis=True\n",
    "             ):\n",
    "\n",
    "    #initialize\n",
    "    stop=False\n",
    "    counter=0\n",
    "    sse=None\n",
    "    \n",
    "    #global mean\n",
    "    if not miu_init:       \n",
    "        miu=arr[~np.isnan(arr)].mean()\n",
    "    else:\n",
    "        miu=miu_init\n",
    "        \n",
    "    #user baseline\n",
    "    if len(b_u_init)==0:\n",
    "        b_u=np.zeros(arr.shape[1])\n",
    "    else:\n",
    "        b_u=b_u_init\n",
    "    \n",
    "    #item baseline\n",
    "    if len(b_i_init)==0:\n",
    "        b_i=np.zeros(arr.shape[0])\n",
    "    else:\n",
    "        b_i=b_i_init\n",
    "        \n",
    "    #user latent factors\n",
    "    if len(p_u_init)==0:\n",
    "        p_u=np.zeros((arr.shape[1],num_of_rank))\n",
    "        p_u.fill(0.1)\n",
    "    else:\n",
    "        p_u=p_u_init\n",
    "    \n",
    "    #item latent factors\n",
    "    if len(q_i_init)==0:\n",
    "        q_i=np.zeros((arr.shape[0],num_of_rank))\n",
    "        q_i.fill(0.1)\n",
    "    else:\n",
    "        q_i=q_i_init\n",
    "    \n",
    "    #gradient descent\n",
    "    while not stop:\n",
    "        \n",
    "        error,b_u,b_i,p_u,q_i=funk_svd_epoch(arr,miu,\n",
    "                                             b_u,b_i,\n",
    "                                             p_u,q_i,\n",
    "                                             alpha,lambda_)\n",
    "\n",
    "        counter+=1\n",
    "\n",
    "        #maximum number of epoch\n",
    "        if counter>=max_iter:\n",
    "            stop=True\n",
    "            if diagnosis:\n",
    "                print('Not converged. Consider increase number of iterations or tolerance')\n",
    "                \n",
    "        #use sum of squared error to determine if converged\n",
    "        sse_prev=sse\n",
    "        sse=error**2\n",
    "        if sse_prev and abs(sse/sse_prev-1)<=tau:\n",
    "            stop=True\n",
    "            if diagnosis:\n",
    "                print(f'{counter} iterations to reach convergence\\n')\n",
    "\n",
    "    return b_u,b_i,p_u,q_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "num_of_latent_factors=40\n",
    "max_num_of_epoch=500\n",
    "learning_rate=0.01\n",
    "lagrange_multiplier=0.02\n",
    "tolerance=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 iterations to reach convergence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#funk svd\n",
    "b_u,b_i,p_u,q_i=funk_svd(arr_train,num_of_rank=num_of_latent_factors,\n",
    "         alpha=learning_rate,\n",
    "         lambda_=lagrange_multiplier,tau=tolerance,\n",
    "         max_iter=max_num_of_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute global mean\n",
    "miu=arr_train[~np.isnan(arr_train)].mean()\n",
    "\n",
    "#matrix completion\n",
    "output=miu+np.repeat(\n",
    "            b_u.reshape(1,-1),\n",
    "            arr_train.shape[0],axis=0)+np.repeat(\n",
    "            b_i.reshape(-1,1),arr_train.shape[1],axis=1)+q_i@p_u.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funk SVD Mean Squared Error: 1.083\n"
     ]
    }
   ],
   "source": [
    "#use mse as benchmark for comparison\n",
    "mse_funk_svd=np.square((\n",
    "    output-arr)[testing_idx]).sum()/len(arr[testing_idx])\n",
    "print('Funk SVD Mean Squared Error:',round(mse_funk_svd,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD++\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Inspired by Funk SVD, more and more variations have been invented to enhance the prediction accuracy and the convergence speed. One of the most famous variation is SVD++ (Koren, 2008). It can be seen as an augmented Funk SVD with user implicit feedback. The accuracy is further improved at the cost of implicit feedback integration. The official optimization problem is formulated as below. The notations are almost the same with only a few extra from implicit feedback.\n",
    "\n",
    "$$ \\min_{p_*,q_*,b_*}\\,\\sum_{r_{ui}\\,\\in\\,\\mathcal{K}} \\left(r_{ui} - \\mu - b_u - b_i - q_i^T\\left(p_u+|N(u)|^{-\\frac{1}{2}} \\sum_{j \\in N(u)}y_j\\right) \\right)^2 + \\lambda\\left( ||p_u||^2 + ||q_i||^2 + \\sum_{j \\in N(u)}||y_j||^2 + b_u^2 + b_i^2 \\right)$$\n",
    "\n",
    "where \n",
    "\n",
    "$N(u)$ denotes the implicit preference list of user $u$\n",
    "\n",
    "$y_j$ denotes the item $j$ that user $u$ implicitly prefers\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "To explain a bit, SVD++ incorporates user implicit feedback into the model. Users may have watched a lot of movies on Netflix yet they choose to rate items in $N(u)$ for a particular reason. For instance, user $u$ may have watched six seasons of House of Cards but he strongly resents the last season. Hence, he gives one star rating to the last season. This kind of implicit feedback is not captured in any latent factors. Nonetheless, the implicit feedback $y_j$ is not independent of any latent factors. Since certain combination $q_i^Tp_u$ triggers the user response and amplifies the extreme rating, $y_j$ is included in both the objective function and the constraint to avoid overfit problem. FYI, $y_j$ can be seen as another latent factor vector with the exact dimension as $q_i$.\n",
    "\n",
    "The actual optimization solver is similar to Funk SVD with one more parameter $y_j$. \n",
    "\n",
    "$$b_u := b_u + \\alpha (\\epsilon_{ui} - \\lambda b_u)$$\n",
    "$$b_i := b_i + \\alpha (\\epsilon_{ui} - \\lambda b_i)$$\n",
    "$$p_u := p_u + \\alpha (\\epsilon_{ui} \\cdot q_i - \\lambda p_u)$$\n",
    "$$q_i := q_i + \\alpha (\\epsilon_{ui} \\cdot (p_u+|N(u)|^{-\\frac{1}{2}} \\sum_{j \\in N(u)}y_j) - \\lambda q_i)$$\n",
    "$$y_j := y_j + \\alpha (|N(u)|^{-\\frac{1}{2}} \\cdot \\epsilon_{ui} \\cdot q_i - \\lambda y_j)$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Reference to the original paper of SVD++ (equation 15)\n",
    "\n",
    "https://people.engr.tamu.edu/huangrh/Spring16/papers_course/matrix_factorization.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use numba to dramatically boost the speed of linear algebra\n",
    "#this will be a lot faster than surprise library\n",
    "@numba.njit\n",
    "def svd_plus_plus_epoch(arr,miu,b_u,b_i,p_u,q_i,y_j,alpha,lambda_):\n",
    "    \n",
    "    #initialize\n",
    "    error=0\n",
    "    \n",
    "    #only iterate known ratings\n",
    "    for i in range(arr.shape[0]):\n",
    "        for u in range(arr.shape[1]):\n",
    "            r_ui=arr[i,u]\n",
    "            \n",
    "            #another way to identify nan\n",
    "            #r_ui!=r_ui\n",
    "            if np.isnan(r_ui):\n",
    "                continue\n",
    "                \n",
    "            #compute implicit feedback\n",
    "            N_u=np.where(~np.isnan(arr[:,u]))[0]\n",
    "            N_u_norm_sqrt=arr[:,u][~np.isnan(arr[:,u])].shape[0]**0.5\n",
    "            feedback=(y_j[N_u,:]/N_u_norm_sqrt).sum(axis=0)\n",
    "\n",
    "            #compute error\n",
    "            epsilon_ui=(r_ui-miu-b_u[u]-b_i[i]-q_i[i].T@(\n",
    "                feedback+p_u[u]).reshape(-1,1)).item()\n",
    "            error+=epsilon_ui\n",
    "\n",
    "            #update\n",
    "            b_u[u]+=alpha*(epsilon_ui-lambda_*b_u[u])\n",
    "            b_i[i]+=alpha*(epsilon_ui-lambda_*b_i[i])\n",
    "            p_u[u]+=alpha*(epsilon_ui*q_i[i]-lambda_*p_u[u])\n",
    "            q_i[i]+=alpha*(epsilon_ui*(p_u[u]+feedback)-lambda_*q_i[i])\n",
    "            y_j[N_u]+=alpha*(epsilon_ui*q_i[N_u]/N_u_norm_sqrt-lambda_*y_j[N_u])\n",
    "               \n",
    "    return error,b_u,b_i,p_u,q_i,y_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svdpp latent factor model\n",
    "def svd_plus_plus(arr,miu_init=None,b_u_init=[],b_i_init=[],\n",
    "             p_u_init=[],q_i_init=[],y_j_init=[],num_of_rank=40,\n",
    "             alpha=0.005,lambda_=0.02,tau=0.0001,\n",
    "             max_iter=20,diagnosis=True\n",
    "             ):\n",
    "\n",
    "    #initialize\n",
    "    stop=False\n",
    "    counter=0\n",
    "    sse=None\n",
    "    \n",
    "    #global mean\n",
    "    if not miu_init:       \n",
    "        miu=arr[~np.isnan(arr)].mean()\n",
    "    else:\n",
    "        miu=miu_init\n",
    "        \n",
    "    #user baseline\n",
    "    if len(b_u_init)==0:\n",
    "        b_u=np.zeros(arr.shape[1])\n",
    "    else:\n",
    "        b_u=b_u_init\n",
    "    \n",
    "    #item baseline\n",
    "    if len(b_i_init)==0:\n",
    "        b_i=np.zeros(arr.shape[0])\n",
    "    else:\n",
    "        b_i=b_i_init\n",
    "        \n",
    "    #user latent factors\n",
    "    if len(p_u_init)==0:\n",
    "        p_u=np.zeros((arr.shape[1],num_of_rank))\n",
    "        p_u.fill(0.1)\n",
    "    else:\n",
    "        p_u=p_u_init\n",
    "    \n",
    "    #item latent factors\n",
    "    if len(q_i_init)==0:\n",
    "        q_i=np.zeros((arr.shape[0],num_of_rank))\n",
    "        q_i.fill(0.1)\n",
    "    else:\n",
    "        q_i=q_i_init\n",
    "        \n",
    "    #user implicit feedback\n",
    "    if len(y_j_init)==0:\n",
    "        y_j=np.zeros((arr.shape[0],num_of_rank))\n",
    "        y_j.fill(0.1)\n",
    "    else:\n",
    "        y_j=y_j_init\n",
    "    \n",
    "    #gradient descent\n",
    "    while not stop:\n",
    "        \n",
    "        error,b_u,b_i,p_u,q_i,y_j=svd_plus_plus_epoch(\n",
    "            arr,miu,b_u,b_i,p_u,q_i,y_j,alpha,lambda_)\n",
    "\n",
    "        counter+=1\n",
    "        \n",
    "        #maximum number of epoch\n",
    "        if counter>=max_iter:\n",
    "            stop=True\n",
    "            if diagnosis:\n",
    "                print('Not converged. Consider increase number of iterations or tolerance')\n",
    "                \n",
    "        #use sum of squared error to determine if converged\n",
    "        sse_prev=sse\n",
    "        sse=error**2\n",
    "        if sse_prev and abs(sse/sse_prev-1)<=tau:\n",
    "            stop=True\n",
    "            if diagnosis:\n",
    "                print(f'{counter} iterations to reach convergence\\n')\n",
    "\n",
    "    return b_u,b_i,p_u,q_i,y_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 iterations to reach convergence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#svdpp\n",
    "b_u,b_i,p_u,q_i,y_j=svd_plus_plus(arr_train,\n",
    "         num_of_rank=num_of_latent_factors,\n",
    "         alpha=learning_rate,\n",
    "         lambda_=lagrange_multiplier,tau=tolerance,\n",
    "         max_iter=max_num_of_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute implicit feedback\n",
    "feedback=[]\n",
    "for u in range(arr_train.shape[1]):\n",
    "    N_u=np.where(~np.isnan(arr_train[:,u]))[0]\n",
    "    N_u_norm_sqrt=arr_train[:,u][~np.isnan(arr_train[:,u])].shape[0]**0.5\n",
    "    feedback.append((y_j[N_u,:]/N_u_norm_sqrt).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix completion\n",
    "output=miu+np.repeat(\n",
    "            b_u.reshape(1,-1),\n",
    "            arr_train.shape[0],axis=0)+np.repeat(\n",
    "            b_i.reshape(-1,1),\n",
    "    arr_train.shape[1],axis=1)+q_i@(p_u+np.mat(feedback)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD++ Mean Squared Error: 1.086\n"
     ]
    }
   ],
   "source": [
    "#use mse as benchmark for comparison\n",
    "mse_svdpp=np.square((\n",
    "    output-arr)[testing_idx]).sum()/len(arr[testing_idx])\n",
    "print('SVD++ Mean Squared Error:',round(mse_svdpp,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory-based\n",
    "\n",
    "#### K Nearest Neighbors\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "KNN is undoubtedly an easy supervised learning model. In the iris dataset, we have shown how KNN works in classification problem. In the case of recommender system, it is effectively a regression problem. We have to slightly change the methodology. \n",
    "\n",
    "* KNN in recommender system takes the weighted average of neighbors to predict the rating, albeit KNN in the iris dataset takes the majority label of neighbors to predict the classification.\n",
    "\n",
    "* KNN in recommender system uses cosine similarity, mean squared distance similarity or Pearson correlation similarity to define neighbors, whereas KNN in the iris dataset uses Euclidean distance, Chebyshev distance or Manhattan distance to define neighbors.\n",
    "\n",
    "In KNN, you also have the choice of picking neighbors based upon the users (how similar users rate the same item) or the items (how similar items get rated by the same user). In this script, every neighborhood model will be based upon the users. For item approach, just switch user and item in the equation below.\n",
    "\n",
    "$$\\hat{r}_{ui} = \\frac{  \\sum_{v \\in N^k_i(u)} \\mathcal{similarity}(u, v) \\cdot r_{vi}}  {\\sum_{v \\in N^k_i(u)} \\mathcal{similarity}(u, v)}$$\n",
    "\n",
    "where \n",
    "\n",
    "$v$ denotes the neighbors of user $u$\n",
    "\n",
    "$N^k_i(u)$ denotes the set of top $k$ neighbors of user $u$ that have rated item $i$\n",
    "\n",
    "$\\mathcal{similarity}(u, v)$ denotes the similarity metric between user $u$ and its neighbor $v$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Each similarity metric has its own pros and cons. In this case, we will pick the MSD similarity which is formulated below. The biggest selling point of MSD is its resemblance to Euclidean distance. One of its malaise is it doesn't penalize a small number of $|I_{uv}|$. A sum of squared error of 0 with $|I_{uv}|$ at 1 is inevitably inferior to a sum of squared error of 1 with $|I_{uv}|$ at 10 but the latter yields a smaller MSD even the latter pair has rated more items in common.\n",
    "\n",
    "$$\\mathcal{similarity}(u, v) = \\frac{1}{\\frac{\\sum_{i \\in I_{uv}} (r_{ui} - r_{vi})^2}{|I_{uv}|}\n",
    "         + 1}$$\n",
    "\n",
    "where \n",
    "\n",
    "$I_{uv}$ denotes the set of items where both user $u$ and its neighbor $v$ have rated\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Reference to KNN in iris dataset\n",
    "\n",
    "https://github.com/je-suis-tm/machine-learning/blob/master/k%20nearest%20neighbors.ipynb\n",
    "\n",
    "Reference to cosine similarity\n",
    "\n",
    "https://github.com/je-suis-tm/machine-learning/blob/master/latent%20semantic%20indexing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain msd similarity matrix\n",
    "@numba.njit\n",
    "def get_msd_similarity_matrix(arr):\n",
    "\n",
    "    similarity_matrix=np.zeros((arr.shape[1],arr.shape[1]))\n",
    "    for u in range(arr.shape[1]):\n",
    "        for v in range(u+1,arr.shape[1]):\n",
    "            \n",
    "            #self correlation distorts knn selection\n",
    "            if u==v:\n",
    "                continue\n",
    "\n",
    "            #compute msd first then eliminate nan\n",
    "            I_uv=np.square(arr[:,u]-arr[:,v])\n",
    "            valid_I_uv=I_uv[~np.isnan(I_uv)]\n",
    "\n",
    "            #avoid the case where two users havent rated any items in common\n",
    "            if len(valid_I_uv)>0:\n",
    "                msd=1/(valid_I_uv.sum()/len(valid_I_uv)+1)\n",
    "            else:\n",
    "                msd=0\n",
    "\n",
    "            #symmetric matrix\n",
    "            similarity_matrix[u,v]=msd\n",
    "            similarity_matrix[v,u]=msd\n",
    "            \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn for matrix completion\n",
    "@numba.njit\n",
    "def knn(arr,similarity_matrix,top_k=10):\n",
    "\n",
    "    estimated=arr.copy()\n",
    "    for i in range(arr.shape[0]):\n",
    "        for u in range(arr.shape[1]):\n",
    "            if np.isnan(arr[i,u]):\n",
    "\n",
    "                #find top k neighbor based upon similarity matrix\n",
    "                rated_users=np.where(~np.isnan(arr[i]))[0]\n",
    "                similarities=similarity_matrix[u][rated_users]\n",
    "                top_k_neighbors=np.argsort(similarities)[-top_k:]\n",
    "                N_k_i_u=np.array([\n",
    "                    rated_users[neighbor] for neighbor in top_k_neighbors])\n",
    "\n",
    "                #compute weighted average                \n",
    "                if len(N_k_i_u)!=0:\n",
    "                    numerator=arr[i][N_k_i_u].T@similarity_matrix[u][N_k_i_u]\n",
    "                    denominator=similarity_matrix[u][N_k_i_u].sum()\n",
    "                    if denominator!=0:\n",
    "                        estimated[i,u]=numerator/denominator\n",
    "                        continue\n",
    "                        \n",
    "                #when the users who rated item i\n",
    "                #have nothing in common with user u\n",
    "                #take the average of user mean and item mean\n",
    "                #if both user mean and item mean are empty\n",
    "                #take global mean\n",
    "                miu_i=arr[i][~np.isnan(arr[i])]\n",
    "                miu_u=arr[:,u][~np.isnan(arr[:,u])]                \n",
    "                if len(miu_i)==0 and len(miu_u)==0:\n",
    "                    estimated[i,u]=arr[np.where(~np.isnan(arr))[0]][\n",
    "                        np.where(~np.isnan(arr))[1]].mean()\n",
    "                else:\n",
    "                    estimated[i,u]=np.array(list(miu_i)+list(miu_u)).mean()\n",
    "                    \n",
    "    return estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute similarity matrix\n",
    "similarity_matrix=get_msd_similarity_matrix(arr_train)\n",
    "\n",
    "#matrix completion\n",
    "output=knn(arr_train,similarity_matrix,top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Mean Squared Error: 1.117\n"
     ]
    }
   ],
   "source": [
    "#use mse as benchmark for comparison\n",
    "mse_knn=np.square((\n",
    "    output-arr)[testing_idx]).sum()/len(arr[testing_idx])\n",
    "print('KNN Mean Squared Error:',round(mse_knn,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors with mean\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Now that we are familiar with KNN, it's about time to introduce many of its variations. The first one is KNN with mean. In the user approach, it takes consideration of user mean and neighbor mean into the equation.\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu_u + \\frac{  \\sum_{v \\in N^k_i(u)} \\mathcal{similarity}(u, v) \\cdot (r_{vi}-\\mu_v)}  {\\sum_{v \\in N^k_i(u)} \\mathcal{similarity}(u, v)}$$\n",
    "\n",
    "where \n",
    "\n",
    "$\\mu_u$ denotes the average rating of user $u$\n",
    "\n",
    "$\\mu_v$ denotes the average rating of neighbor $v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn with mean for matrix completion\n",
    "@numba.njit\n",
    "def knn_with_mean(arr,similarity_matrix,top_k=10):\n",
    "    \n",
    "    #compute user mean\n",
    "    user_mean=np.array([arr[:,u][\n",
    "        ~np.isnan(arr[:,u])].mean() for u in range(arr.shape[1])])\n",
    "\n",
    "    estimated=arr.copy()\n",
    "    for i in range(arr.shape[0]):\n",
    "        for u in range(arr.shape[1]):\n",
    "            if np.isnan(arr[i,u]):\n",
    "\n",
    "                #find top k neighbor based upon similarity matrix\n",
    "                rated_users=np.where(~np.isnan(arr[i]))[0]\n",
    "                similarities=similarity_matrix[u][rated_users]\n",
    "                top_k_neighbors=np.argsort(similarities)[-top_k:]\n",
    "                N_k_i_u=np.array([\n",
    "                    rated_users[neighbor] for neighbor in top_k_neighbors])\n",
    "\n",
    "                #compute weighted average                \n",
    "                if len(N_k_i_u)!=0:\n",
    "                    numerator=(arr[i][\n",
    "                        N_k_i_u]-user_mean[\n",
    "                        N_k_i_u]).T@similarity_matrix[u][N_k_i_u]\n",
    "                    denominator=similarity_matrix[u][N_k_i_u].sum()\n",
    "                    if denominator!=0:\n",
    "                        estimated[i,u]=user_mean[u]+numerator/denominator\n",
    "                        continue\n",
    "                        \n",
    "                #when the users who rated item i\n",
    "                #have nothing in common with user u\n",
    "                #take the average of user mean and item mean\n",
    "                #if both user mean and item mean are empty\n",
    "                #take global mean\n",
    "                miu_i=arr[i][~np.isnan(arr[i])]\n",
    "                miu_u=arr[:,u][~np.isnan(arr[:,u])]                \n",
    "                if len(miu_i)==0 and len(miu_u)==0:\n",
    "                    estimated[i,u]=arr[np.where(~np.isnan(arr))[0]][\n",
    "                        np.where(~np.isnan(arr))[1]].mean()\n",
    "                else:\n",
    "                    estimated[i,u]=np.array(list(miu_i)+list(miu_u)).mean()\n",
    "                    \n",
    "    return estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix completion\n",
    "output=knn_with_mean(arr_train,similarity_matrix,top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with μ Mean Squared Error: 1.07\n"
     ]
    }
   ],
   "source": [
    "#use mse as benchmark for comparison\n",
    "mse_knn_with_miu=np.square((\n",
    "    output-arr)[testing_idx]).sum()/len(arr[testing_idx])\n",
    "print('KNN with μ Mean Squared Error:',round(mse_knn_with_miu,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors with z score\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "On top of KNN with mean, we can include the standard deviation of user into the equation. After all, some users may be moody so their ratings tend to have a larger fluctuation.\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu_u + \\sigma_u \\cdot \\frac{  \\sum_{v \\in N^k_i(u)} \\mathcal{similarity}(u, v) \\cdot \\frac {(r_{vi}-\\mu_v)} {\\sigma_v}  }  {\\sum_{v \\in N^k_i(u)} \\mathcal{similarity}(u, v) } $$\n",
    "\n",
    "where \n",
    "\n",
    "$\\sigma_u$ denotes the standard deviation of the rating from user $u$\n",
    "\n",
    "$\\sigma_v$ denotes the standard deviation of the rating from neighbor $v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn with z score for matrix completion\n",
    "@numba.njit\n",
    "def knn_with_zscore(arr,similarity_matrix,top_k=10):\n",
    "    \n",
    "    #compute user mean and std\n",
    "    user_mean=np.array([arr[:,u][\n",
    "        ~np.isnan(arr[:,u])].mean() for u in range(arr.shape[1])])\n",
    "    user_std=np.array([arr[:,u][\n",
    "        ~np.isnan(arr[:,u])].std() for u in range(arr.shape[1])])\n",
    "\n",
    "    estimated=arr.copy()\n",
    "    for i in range(arr.shape[0]):\n",
    "        for u in range(arr.shape[1]):\n",
    "            if np.isnan(arr[i,u]):\n",
    "\n",
    "                #find top k neighbor based upon similarity matrix\n",
    "                rated_users=np.where(~np.isnan(arr[i]))[0]\n",
    "                similarities=similarity_matrix[u][rated_users]\n",
    "                top_k_neighbors=np.argsort(similarities)[-top_k:]\n",
    "                N_k_i_u=np.array([\n",
    "                    rated_users[neighbor] for neighbor in top_k_neighbors])\n",
    "\n",
    "                #compute weighted average                \n",
    "                if len(N_k_i_u)!=0:\n",
    "                    z_score=np.divide((arr[i][N_k_i_u]-user_mean[N_k_i_u]),\n",
    "                                      user_std[N_k_i_u])\n",
    "                    numerator=z_score.T@similarity_matrix[u][N_k_i_u]\n",
    "                    denominator=similarity_matrix[u][N_k_i_u].sum()\n",
    "                    if denominator!=0:\n",
    "                        estimated[i,u]=user_mean[u]+user_std[\n",
    "                            u]*numerator/denominator\n",
    "                        continue\n",
    "                        \n",
    "                #when the users who rated item i\n",
    "                #have nothing in common with user u\n",
    "                #take the average of user mean and item mean\n",
    "                #if both user mean and item mean are empty\n",
    "                #take global mean\n",
    "                miu_i=arr[i][~np.isnan(arr[i])]\n",
    "                miu_u=arr[:,u][~np.isnan(arr[:,u])]                \n",
    "                if len(miu_i)==0 and len(miu_u)==0:\n",
    "                    estimated[i,u]=arr[np.where(~np.isnan(arr))[0]][\n",
    "                        np.where(~np.isnan(arr))[1]].mean()\n",
    "                else:\n",
    "                    estimated[i,u]=np.array(list(miu_i)+list(miu_u)).mean()\n",
    "                    \n",
    "    return estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix completion\n",
    "output=knn_with_zscore(arr_train,similarity_matrix,top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with Z Score Mean Squared Error: 1.095\n"
     ]
    }
   ],
   "source": [
    "#use mse as benchmark for comparison\n",
    "mse_knn_with_zscore=np.square((\n",
    "    output-arr)[testing_idx]).sum()/len(arr[testing_idx])\n",
    "print('KNN with Z Score Mean Squared Error:',round(mse_knn_with_zscore,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "154px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
